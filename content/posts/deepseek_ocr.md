---
title: 딥시크가 GPU를 1/10로 또 줄였대요. 네, 0 하나 뺐다고요. 😱
date: 2025-10-21T09:45:40+09:00
lastmod: 2025-10-21T09:45:40+09:00
# author: Author Name
# authorlink: https://author.site
avatar: 'https://cdn-icons-png.flaticon.com/512/2118/2118520.png'
cover: https://github.com/deepseek-ai/DeepSeek-OCR/raw/main/assets/logo.svg
images:
  - https://github.com/deepseek-ai/DeepSeek-OCR/raw/main/assets/logo.svg
categories:
  - AI 트렌드
tags:
  - 딥시크
  - 중국
  - LLM
  - GPU 효율화
  - 비전 토큰
nolastmod: true
draft: false
---

혹시 AI 챗봇에게 아주 긴 문서나 두꺼운 보고서를 주고 "이거 요약해 줘"라고 시켜본 적 있으신가요? <br>AI는 만능처럼 보이지만, 사실 텍스트가 길어질수록 AI도 우리처럼 엄청나게 지쳐버립니다. 텍스트를 처리하는 데 필요한 시간과 컴퓨터 자원(GPU)이 기하급수적으로 늘어나기 때문인데요.<br>

이 문제를 해결하기 위해 DeepSeek-AI 연구팀이 마치 우리가 속독하듯이, <br>긴 텍스트 문서를 **이미지**로 바꿔 압축하는 'DeepSeek-OCR'이라는 획기적인 기술을 개발했습니다. 이 방법 덕분에 AI는 문서를 훨씬 적은 데이터 조각만으로 처리할 수 있게 되어, 자원 소모를 최대 10분의 1까지 줄일 수 있게 되었어요

[딥시크 OCR Github 바로가기](https://github.com/deepseek-ai/DeepSeek-OCR)


---


## 1. AI가 긴 글을 읽을 때 'GPU가 비명을 지르는' 이유

AI, 특히 챗GPT와 같은 대규모 언어 모델(LLM)은 우리가 주는 텍스트를 '**토큰(Token)**'이라는 작은 단위로 쪼개서 처리합니다. 예를 들어, "나는 학교에 간다"라는 문장은 "나", "는", "학교", "에", "간다"처럼 나뉠 수 있죠.

문제는 AI가 이 토큰들의 '문맥'을 파악하는 방식에 있습니다. AI는 문장 속 모든 토큰이 다른 모든 토큰과 어떤 관계가 있는지 '**서로 일일이 대조**'합니다.

* 100개의 토큰(짧은 문단)을 처리하는 것은 100 x 100 = 10,000번의 계산이 필요합니다.
* 1,000개의 토큰(긴 문서 한 페이지)을 처리하려면 1,000 x 1,000 = 1,000,000번의 계산이 필요합니다.

텍스트 길이가 10배 늘어났을 뿐인데, 계산량(컴퓨팅 자원)은 그 제곱인 **100배**로 폭증합니다. 😱 (이를 기술 용어로 '**제곱 비례 복잡도(Quadratic Scaling)**'라고 부릅니다.)

이것이 바로 AI에게 긴 문서를 맡기면 GPU가 과열되고 비용이 기하급수적으로 증가하는 근본적인 이유입니다.

## 2. DeepSeek-OCR의 역발상: "텍스트를 텍스트로 읽지 말자"

AI 업계는 이 문제를 해결하기 위해 애썼습니다. 고해상도 문서를 퍼즐처럼 조각내어 처리해봤지만(타일 기반), 조각(토큰) 수가 너무 많아졌습니다. 통째로 처리하려니 GPU 메모리가 터져버렸죠.

여기서 **DeepSeek-OCR**은 아주 영리한 역발상을 제시합니다.

> "1,000개의 텍스트 토큰을 하나하나 분석하지 말고,
> 그냥 그 1,000단어가 담긴 **페이지 전체를 '사진 한 장'으로 찍어서 AI에게 보여주자!**"

텍스트가 아닌, 텍스트가 담긴 '**시각적 양식(Visual Modality)**'으로 접근한 것입니다. AI는 이 '사진'을 **딥 인코더 DeepEncoder**라는 강력한 압축 엔진을 통해 훨씬 적은 수의 '**비전 토큰(Vision Token)**', 즉 'AI가 이미지를 이해하는 데 필요한 핵심 시각 정보 조각'으로 변환합니다.

---

### 사례 1: 10배 압축하고도 97% 정확도를 지킨 '광학적 압축'

이 방식이 얼마나 효율적인지 실제 테스트 결과를 보면 놀랍습니다.

**📊 텍스트 토큰 vs 비전 토큰 압축 효율 (Fox 벤치마크)**

![이미지1](https://github.com/deepseek-ai/DeepSeek-OCR/blob/main/assets/fig1.png?raw=true)

| 원본 텍스트 토큰 수 (AI가 읽어야 할 글자 수) | 압축 후 비전 토큰 수 (AI가 본 '정보 조각' 수) | 압축률 | OCR 해독 정확도 |
| :--- | :--- | :--- | :--- |
| 600 ~ 700개 (긴 문서) | 100개 (Small Mode) | 6.7배 | **98.5%** |
| 800 ~ 900개 (매우 긴 문서) | 100개 (Small Mode) | 8.5배 | **96.8%** |
| 1,200 ~ 1,300개 (초고밀도 문서) | 64개 (Tiny Mode) | 19.7배 | 약 59.1% |

**이 표가 우리에게 말해주는 것:**

* **일반일을 위한 해석:**
    800 ~ 900단어짜리 빽빽한 문서를 AI가 텍스트로 읽으려면 800 ~ 900개의 조각을 처리해야 했습니다. 하지만 DeepSeek-OCR은 이 페이지를 '사진'으로 찍어 **단 100개**의 '시각 정보 조각'으로 압축했습니다. 8.5배나 압축했는데도, 원본 내용을 96.8%나 정확하게 복원해냈습니다. 10배 빠른 AI '속독'이 가능해진 것입니다!

* **전문가를 위한 통찰:**
    GPU 효율은 단순 8.5배가 아닙니다. 기존 방식의 계산량이 O(n^2) (여기서 n=900)였다면, 새 방식은 O(k^2) (여기서 k=100)입니다. 즉, 900^2 (810,000) 대비 100^2 (10,000)로, **이론상 계산 부하를 최대 81배까지 줄일 수 있는 잠재력**을 보여줍니다. 또한, 19.7배의 초고압축(Tiny Mode)에서는 정확도가 59.1%로 떨어지는 것을 통해, 압축률과 정확도 간의 상충 관계(trade-off)를 보여주는 구체적인 기술 지표(empirical guideline)를 제공합니다.

---

### 사례 2: 단순 '읽기(OCR 1.0)'를 넘어 '구조적 이해(OCR 2.0)'로

DeepSeek-OCR의 진정한 가치는 단순히 글자를 빨리 읽는 것을 넘어섭니다. 이미지 속의 '구조'를 이해하는 '**딥 파싱(Deep Parsing)**' 능력을 갖추고 있기 때문입니다.

![이미지1](https://github.com/deepseek-ai/DeepSeek-OCR/blob/main/assets/show2.jpg?raw=true)

**1. 금융/과학: 이미지 속 '차트'를 '엑셀 데이터'로 변환**
* **문제:** 보고서 속의 막대그래프나 파이 차트는 단순한 '그림'이어서 데이터를 복사할 수 없었습니다.
* **해결:** DeepSeek-OCR은 이 차트를 '보고' 그 안의 데이터를 인식하여, 우리가 즉시 엑셀이나 구글 시트에 붙여넣을 수 있는 **HTML 테이블 형식**으로 변환해줍니다.


**2. 화학/STEM: '화학 구조식'을 '데이터베이스 언어'로 변환**
* **문제:** 논문 속의 복잡한 분자 구조식 이미지는 AI에게 그냥 '그림'일 뿐이었습니다.
* **해결:** 이 모델은 화학 구조식을 AI가 이해하고 데이터베이스에서 검색할 수 있는 표준 언어인 **SMILES 형식**으로 자동 변환합니다. AI가 이미지를 보고 "이 물질은 벤젠 고리를 포함하고 있군"이라고 분석할 수 있게 된 것입니다.

![이미지2](https://github.com/deepseek-ai/DeepSeek-OCR/blob/main/assets/show1.jpg?raw=true)

**3. 교육/기술: '수학 도형'의 관계 이해**
* **문제:** 수학 문제지의 복잡한 도형(예: 직각삼각형, 원의 접선)은 그 관계를 파악하기 어렵습니다.
* **해결:** DeepSeek-OCR은 이 도형들의 구조적 관계(예: '선 AB'와 '선 CD'가 직각으로 만난다)를 인식하고 파싱할 수 있습니다.

![이미지2](https://github.com/deepseek-ai/DeepSeek-OCR/blob/main/assets/show4.jpg?raw=true)

---

### 사례 3: AI의 '기억 관리법' (미래 비전)

이 '광학적 압축' 기술은 AI의 장기 기억력을 관리하는 방식에도 혁신적인 아이디어를 제공합니다. 바로 인간의 '**망각 메커니즘**'을 모방하는 것입니다.

우리의 기억도 방금 겪은 일은 생생하고(고해상도), 1년 전 일은 핵심만 남고 흐릿해지(저해상도/요약)는 것처럼 말이죠.

1.  **최신 기억 (고해상도):**
    AI가 방금 나눈 대화나 최근에 읽은 문서는 정보 손실이 없도록 **선명한 고해상도 이미지**로 렌더링하여 정확하게 보관합니다. (낮은 압축률)

2.  **오래된 기억 (점진적 압축):**
    시간이 지난 오래된 대화 기록이나 문서는 어떨까요? AI가 이 기록을 이미지로 렌더링한 뒤, **해상도를 점진적으로 낮추어(흐릿하게 만들어) 압축률을 높입니다.** (높은 압축률, 적은 토큰)

이렇게 하면, AI는 한정된 GPU 자원 내에서도 **최신 정보는 날카롭게, 오래된 정보는 효율적으로** 보관하며 '**이론상 무제한의 대화 기록(Context)**'을 관리할 수 있게 됩니다.



---

## 이 기술이 나와 무슨 상관이죠? 

DeepSeek-OCR이 제시한 '광학적 압축'은 단순히 신기한 기술을 넘어, LLM이 직면한 가장 큰 벽인 '컴퓨팅 비용' 문제를 해결할 강력한 열쇠입니다.

**✅ 사용자가 얻을 이득:**
* **더 빠르고 저렴한 AI:** AI가 긴 PDF, 보고서, 심지어 손글씨 메모까지 '속독'할 수 있게 되어, 우리가 AI 서비스를 이용하는 비용이 훨씬 저렴해질 수 있습니다.
* **더 똑똑한 AI:** 단순 텍스트를 넘어 차트, 도표, 화학식까지 '이해'하는 AI가 여러분의 업무와 학습을 더 깊이 있게 도울 수 있습니다.
* **기억력 좋은 AI:** AI가 대화의 맥락을 훨씬 더 오래 기억하여(마치 '망각 메커니즘'처럼), 더 자연스럽고 연속적인 대화가 가능해집니다.

**✅ 개발자가 얻을 통찰:**
* **O(n^2) 병목 현상의 실용적 우회로:** 텍스트 모달리티의 제곱 비례 복잡도 문제를 시각 모달리티로 전환하여 O(k^2) (k « n)로 해결하는 강력한 아키텍처적 대안을 제시합니다.
* **고속 데이터 생성 파이프라인:** 이 기술을 활용하면 **단일 A100 GPU로 하루 20만 페이지** 이상의 고품질 학습 데이터를 생성할 수 있습니다. 이는 VLM/LLM 사전 학습 데이터 구축에 드는 시간과 비용을 획기적으로 줄여줍니다.
* **구체적인 최적화 지표 획득:** 1,000 토큰 미만의 문서는 64~100개의 비전 토큰으로도 충분하다는 실험 결과는, 향후 VLM 설계 시 토큰 최적화를 위한 명확한 기술적 가이드라인(empirical guidelines)이 됩니다.

DeepSeek-OCR은 우리가 AI의 연산 능력 한계에 부딪혔을 때, "한 걸음 물러서서 문제를 바라보는" 창의적인 접근 방식이 얼마나 강력한지를 보여주는 멋진 사례입니다.


---

## [ 쉬어가기 ] O(n^2) 이 도대체 무슨 뜻일까요?

AI 관련 글을 읽다 보면 O(n^2) (빅오-엔-스퀘어)라는 암호 같은 기호를 자주 마주치게 됩니다. 이건 "작업량이 입력값(n)의 '제곱'에 비례해 폭발한다"는 뜻으로, '제곱 비례 복잡도'라고 부릅니다.

이걸 '파티의 악수 문제'에 비유하면 아주 쉽습니다.

* 파티에 3명(n=3)이 있습니다. 모든 사람이 서로 한 번씩 악수하려면 총 몇 번의 악수가 필요할까요?
    * A가 B, C와 악수 (2번)
    * B가 C와 악수 (1번)
    * **총 3번**의 악수가 필요합니다.

* 그런데 파티에 10명(n=10)이 있다면 어떨까요?
    * 1번 사람이 9명과 악수
    * 2번 사람이 8명과 악수
    * ...
    * 9번 사람이 1명과 악수
    * **총 45번**의 악수가 필요합니다.

사람 수(n)는 약 **3.3배** (3명에서 10명) 늘었을 뿐인데, 해야 할 일(악수)은 **15배**(3번에서 45번으로)나 늘어났습니다.

만약 파티에 **100명**(n=100)이 모인다면, 악수는 **총 4,950번** 필요합니다. <br>
입력값(n)이 10배(10명에서 100명) 늘어나자, 작업량은 약 **100배**(45번에서 4950번으로) 가까이 폭증했습니다.

이것이 바로 O(n^2) 의 무서움입니다. 

LLM(AI)의 세계에서 '사람(n)'은 '단어 토큰 수'이고, '악수'는 '문맥을 파악하기 위한 계산'입니다. <br>
1,000개의 단어(토큰)를 처리한다는 것은, 1,000명의 사람이 파티에서 서로 악수하는 것과 비슷하게 1000 x 1000 = 1,000,000 (백만)에 비례하는 막대한 계산이 필요하다는 뜻입니다. <br><br>

DeepSeek-OCR은 이 '사람 수(n)' 자체를 획기적으로 줄여(1000명에서 100명), 계산량을 100배 줄여버린 기술입니다.<br><br>
다음번에도 AI가 세상을 어떻게 바꾸고 있는지, 더 흥미롭고 유익한 소식으로 찾아뵙겠습니다. 
